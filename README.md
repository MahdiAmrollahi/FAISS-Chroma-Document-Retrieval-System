# Vector Database Tools

Simple RAG system with FAISS and ChromaDB for document indexing and semantic search.

## Quick Start

```bash
# Create virtual environment (Windows)
python -m venv env
env\Scripts\activate

# macOS/Linux
python -m venv env
source env/bin/activate

# Install dependencies
pip install -r requirements.txt

# Index documents
python index.py

# Search
python query.py "your query here"
```

## Project Structure

- **`index.py`** – Runs the full indexing pipeline (load → chunk → embed → store)
- **`query.py`** – CLI search tool (FAISS + ChromaDB backends)
- **`vector_db_tools/`** – Reusable building blocks:
  - `basic.py` – File loader + sentence-level chunker (returns text + metadata)
  - `faiss_store.py` – Vector normalization, index build/read, similarity search
  - `chromadb_store.py` – Persistent ChromaDB client, normalized inserts, metadata-aware queries
  - `search.py` – High-level search helpers (`search` + `search_text`)
- **`data/`** – Put source documents here (PDF, DOCX, TXT, Markdown, …).  
  Each file is chunked with overlap; metadata keeps track of filename + path.  
  You can add/remove files freely—rerun `index.py` to rebuild embeddings.
- **`chroma_db/`** – Auto-generated persistent store (safe to delete when reindexing)
- **`vector_index.faiss`** – Saved FAISS index (regenerated by `index.py`)

## Configuration

### Chunking (easy to tweak)
`vector_db_tools/basic.py`
```python
chunk_size = 512     # Larger chunks → more context per vector
chunk_overlap = 50   # Overlap helps preserve sentence continuity
```

### Embedding Model (swap any time)
`index.py` + `query.py`
```python
model = SentenceTransformer("BAAI/bge-small-en-v1.5")
```

### Data Directory (expandable)
`index.py`
```python
texts = load_files("data")  # Point to another folder or S3 mount
```

### Collection / Index Names
`vector_db_tools/chromadb_store.py`
```python
collection_name = "documents"  # Rename per project/environment
```

## FAISS Index Types

Currently using **IndexFlatL2** (brute force, 100% accurate but slower).

### Available Options:

**1. Flat Indexes** (Current)
- `IndexFlatL2` - Euclidean distance
- `IndexFlatIP` - Inner product (cosine similarity)

**2. IVF (Inverted File)**
- `IndexIVFFlat` - Fast for large datasets
- `IndexIVFPQ` - Industry standard for millions of vectors

**3. HNSW (Graph-Based)**
- `IndexHNSWFlat` - Very fast, high accuracy
- Best for 1M-100M vectors

**4. Product Quantization**
- `IndexPQ` - Memory efficient
- `IndexIVFPQ` - Most common in production

To change index type, edit `vector_db_tools/faiss_store.py`:
```python
# Current: IndexFlatL2
index = faiss.IndexFlatL2(dimension)

# For HNSW:
index = faiss.IndexHNSWFlat(dimension, 32)  # 32 = M parameter

# For IVF:
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, 100)  # 100 = nlist
index.train(vectors)
```

## ChromaDB

- **Persistent storage** in `./chroma_db/`
- **Metadata** includes source filename and file path
- **Normalized embeddings** for better similarity search
- **Collection name**: `documents` (configurable)

### ChromaDB Features:
- Built-in document storage (no need for separate chunk mapping)
- Automatic embedding normalization
- Metadata filtering support
- SQLite backend (portable)
- Good for small to medium datasets (< 100M vectors)

## Features

✅ Normalized embeddings (L2) for better accuracy  
✅ Metadata tracking (filename, file path)  
✅ Dual backend support (FAISS + ChromaDB)  
✅ Unified search interface  

## FAISS vs ChromaDB

| Feature | FAISS | ChromaDB |
|---------|-------|----------|
| **Speed** | Very fast (especially HNSW/IVF) | Fast |
| **Memory** | Efficient (PQ compression) | Higher |
| **Documents** | Only vectors (need mapping) | Built-in storage |
| **Metadata** | Manual handling | Native support |
| **Scalability** | Billions of vectors | Millions |
| **Best For** | Large-scale production | RAG apps, prototyping |

## Notes

- FAISS returns indices only (needs chunks for text display)
- ChromaDB stores documents automatically
- Both use normalized vectors for cosine similarity
